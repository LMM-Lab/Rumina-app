import asyncio
import base64
import json
import uuid
from pathlib import Path

import numpy as np
import scipy.io.wavfile as wav
import scipy.io.wavfile as wavfile
from api.utils.invalid_transcription import is_invalid_transcription
from api.utils.model_selector import (
    get_multimodal_response_func,
    get_transcriber_instance,
    get_tts_instance,
)
from fastapi import APIRouter, WebSocket, WebSocketDisconnect

router = APIRouter()

MAX_HISTORY = 5
conversation_history = []


def save_debug_wav(
    pcm_bytes: bytes, sample_rate: int = 16000, prefix: str = "debug_audio"
):
    """ãƒ‡ãƒãƒƒã‚°ç”¨ã« PCM ãƒ‡ãƒ¼ã‚¿ã‚’ WAV ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
    try:
        save_path = f"{prefix}_{uuid.uuid4().hex[:8]}.wav"
        audio_np = np.frombuffer(pcm_bytes, dtype=np.int16)
        wav.write(save_path, sample_rate, audio_np)
        print(f"ğŸ’¾ ãƒ‡ãƒãƒƒã‚°éŸ³å£°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")
    except Exception as e:
        print(f"âš ï¸ save_debug_wav ã‚¨ãƒ©ãƒ¼: {e}")


@router.websocket("/ws/m/image")
async def we_image_endpoint(websocket: WebSocket):
    await websocket.accept()
    print("âœ… WebSocket æ¥ç¶šã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸ")

    # STEP 1ï¸âƒ£ ãƒ¢ãƒ‡ãƒ«åã‚’æœ€åˆã«å—ã‘å–ã‚‹
    init_data = await websocket.receive_json()
    model_name = init_data.get("model", "rumina-m2")
    print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«åã‚’å—ã‘å–ã‚Šã¾ã—ãŸ: {model_name}")
    vad_silence_threshold_ms = init_data.get("vad_silence_threshold", 1000)
    print(f"âœ… VAD silence threshold {vad_silence_threshold_ms} ms")

    # STEP 2ï¸âƒ£ ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ãŸã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹é¸æŠ
    transcriber_instance = get_transcriber_instance(model_name)
    print("now")
    # æ–‡å­—èµ·ã“ã—å‡¦ç†é–‹å§‹
    await transcriber_instance.start()
    multimodal_response_func = get_multimodal_response_func(model_name)
    m_tts_instance = get_tts_instance(model_name)

    transcriber_instance.set_silence_threshold(
        (vad_silence_threshold_ms / 1000.0) - 0.3  # ä½™è£•ã‚’æŒãŸã›ã‚‹
    )

    # â˜… éŸ³å£°ãƒãƒƒãƒ•ã‚¡
    audio_buffer = bytearray()

    async def receive_audio_and_image():
        while True:
            try:
                message = await websocket.receive()

                if message["type"] == "websocket.receive":
                    if "text" in message:
                        data = message["text"]
                        # print(f"ğŸŸ¢ å—ä¿¡ JSON ãƒ‡ãƒ¼ã‚¿: {data}")
                        data_json = json.loads(data)

                        if data_json["type"] == "active_audio_start":
                            print("ğŸ™ï¸ START")
                            audio_buffer.clear()

                            image_base64 = data_json.get("image_base64")
                            if image_base64:
                                print("ğŸ–¼ï¸ ç”»åƒãƒ‡ãƒ¼ã‚¿å—ä¿¡ â†’ æ›´æ–°")
                                transcriber_instance.update_latest_image(
                                    image_base64
                                )
                            else:
                                print("âš ï¸ image_base64 ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

                        elif data_json["type"] == "active_audio_end":
                            print("ğŸ›‘ END, processing audio")
                            # save_debug_wav(
                            #     bytes(audio_buffer), prefix="debug_end"
                            # )
                            # print(
                            #     f"ğŸ“¤ audio_buffer size = {len(audio_buffer)} bytes"
                            # )

                            # ã“ã“ã§ã®ã¿ transcribe_audio_chunk å‘¼ã¶
                            await transcriber_instance.transcribe_audio_chunk(
                                bytes(audio_buffer)
                            )
                            print(f"âœ… transcribe_audio_chunk å‘¼ã³å‡ºã—å®Œäº†")

                    elif "bytes" in message:
                        pcm_bytes = message["bytes"]
                        # print(f"ğŸµ Received {len(pcm_bytes)} bytes (binary)")
                        audio_buffer.extend(pcm_bytes)
                        # print(
                        #     f"ğŸ”„ audio_buffer size = {len(audio_buffer)} bytes"
                        # )

                        # âœ… ãƒ‡ãƒãƒƒã‚°ç”¨ä¿å­˜ (Optional)
                        # if (
                        #     len(audio_buffer) >= 1024
                        #     and len(audio_buffer) % 1024 == 0
                        # ):
                        # save_debug_wav(
                        #     bytes(audio_buffer), prefix="debug_chunk"
                        # )
                        # print(
                        #     f"ğŸ’¾ ä¸€æ™‚ãƒ‡ãƒãƒƒã‚°ä¿å­˜ (size: {len(audio_buffer)} bytes)"
                        # )

            except Exception as e:
                print(f"ğŸš¨ receive_audio_and_image ã‚¨ãƒ©ãƒ¼: {e}")
                break

    async def transcription_to_response_pipeline():
        while True:
            try:
                # WhisperSimpleTranscriber ã«å¯¾å¿œ
                transcription = await transcriber_instance.result_queue.get()
                print(
                    f"ğŸ“ result_queue.get() â†’ {type(transcription)}, å†…å®¹: {transcription}"
                )

                # transcription ãŒ dict ãªã‚‰ text ã‚’å–ã‚Šå‡ºã™ã€str ãªã‚‰ãã®ã¾ã¾ä½¿ã†
                if isinstance(transcription, dict):
                    transcribed_text = transcription.get("text", "")
                elif isinstance(transcription, str):
                    transcribed_text = transcription
                else:
                    transcribed_text = ""
                    print(
                        f"âš ï¸ transcription ã®æƒ³å®šå¤–ã®å‹: {type(transcription)} â†’ å†…å®¹: {transcription}"
                    )

                # ç„¡åŠ¹ãƒã‚§ãƒƒã‚¯
                if is_invalid_transcription(transcribed_text):
                    print("â­ï¸ ç„¡åŠ¹ãªæ–‡å­—èµ·ã“ã—ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼š", transcribed_text)
                    continue

                # transcription é€ä¿¡
                await websocket.send_json(
                    {"type": "transcription", "message": transcribed_text}
                )

                # å±¥æ­´æ§‹ç¯‰
                history_messages = [
                    {
                        "role": "system",
                        "content": "You are a helpful assistant.",
                    }
                ]
                history_messages.extend(
                    conversation_history[-(MAX_HISTORY * 2) :]
                )
                history_messages.append(
                    {"role": "user", "content": transcribed_text}
                )

                latest_image_base64 = transcriber_instance.latest_image_base64

                # ãƒ¢ãƒ‡ãƒ«ã«å•ã„åˆã‚ã›
                response = await asyncio.to_thread(
                    multimodal_response_func,
                    message=transcribed_text,
                    image_base64=latest_image_base64,
                    history=history_messages,
                )

                # # TTS â†’ base64éŸ³å£°
                audio_base64 = await asyncio.to_thread(
                    m_tts_instance.synthesize_to_base64, response
                )

                # å¿œç­”é€ä¿¡
                await websocket.send_json(
                    {
                        "type": "ai_response",
                        "message": response,
                        "audio_base64": audio_base64,
                    }
                )

                # å±¥æ­´è¿½åŠ 
                conversation_history.append(
                    {"role": "user", "content": transcribed_text}
                )
                conversation_history.append(
                    {"role": "assistant", "content": response}
                )

            except Exception as e:
                print(f"ğŸš¨ transcription_to_response_pipeline ã‚¨ãƒ©ãƒ¼: {e}")
                break

    # ä¸¦è¡Œã‚¿ã‚¹ã‚¯èµ·å‹•
    receive_task = asyncio.create_task(receive_audio_and_image())
    send_task = asyncio.create_task(transcription_to_response_pipeline())

    try:
        await asyncio.gather(receive_task, send_task)
    except WebSocketDisconnect as e:
        print("âŒ WebSocket æ¥ç¶šãŒåˆ‡æ–­ã•ã‚Œã¾ã—ãŸã€‚ã‚³ãƒ¼ãƒ‰:", e.code)
    except Exception as e:
        print("ğŸš¨ æƒ³å®šå¤–ã®ã‚¨ãƒ©ãƒ¼:", e)
    finally:
        receive_task.cancel()
        send_task.cancel()
        await transcriber_instance.stop()
        print("ğŸ›‘ éŒ²éŸ³ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†")
