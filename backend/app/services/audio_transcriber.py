import asyncio
import re
import time
from collections import Counter
from tempfile import NamedTemporaryFile

import numpy as np
import torch
import scipy.io.wavfile as wav
import webrtcvad
import whisper
from numpy.typing import NDArray


class WhisperAudioTranscriber:
    def __init__(self, sample_rate: int = 16000, use_vad: bool = True):
        self.sample_rate = sample_rate
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = whisper.load_model("base", device=device)
        # éåŒæœŸå‡¦ç†ç”¨ã® asyncio.Queue ã‚’ä½¿ç”¨
        self.audio_queue = asyncio.Queue()
        # æ–‡å­—èµ·ã“ã—çµæœã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é€ä¿¡ã™ã‚‹ãŸã‚ã®ã‚­ãƒ¥ãƒ¼
        self.result_queue = asyncio.Queue()
        self.use_vad = use_vad

        # RMS åˆ¤å®šç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.silence_rms_threshold = 0.01
        self.silence_duration_threshold = 0.3

        # VAD åˆ¤å®šç”¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆãƒ¢ãƒ¼ãƒ‰ 2: ä¸­ç¨‹åº¦ã®å³ã—ã•ï¼‰
        self.vad = webrtcvad.Vad(2)
        self.frame_duration_ms = 30  # 30ms
        self.frame_size = int(sample_rate * self.frame_duration_ms / 1000)

        self._running = False
        self._task: asyncio.Task[None] | None = None

        self.latest_image_base64 = None
        self.result_bundle_queue = asyncio.Queue()

    def compute_rms(self, data: NDArray[np.float32]) -> float:
        return np.sqrt(np.mean(np.square(data)))

    def is_speech_vad(self, pcm_chunk: NDArray[np.int16]) -> bool:
        if pcm_chunk.dtype != np.int16:
            pcm_chunk = (pcm_chunk * 32767).astype(np.int16)
        pcm_bytes = pcm_chunk.tobytes()
        try:
            return self.vad.is_speech(pcm_bytes, sample_rate=self.sample_rate)
        except Exception as e:
            print(f"VADã‚¨ãƒ©ãƒ¼: {e}")
            return False

    async def put_audio_chunk(self, chunk: bytes):
        np_chunk = (
            np.frombuffer(chunk, dtype=np.int16).astype(np.float32) / 32768.0
        )
        await self.audio_queue.put(np_chunk)

    def update_latest_image(self, image_base64: str):
        self.latest_image_base64 = image_base64

    async def start(self):
        if not self._running:
            print("â–¶ï¸ Improved transcription loop ã‚’é–‹å§‹")
            self._running = True
            self._task = asyncio.create_task(self.transcription_loop())

    async def stop(self):
        print("â›” Improved transcription loop ã‚’åœæ­¢")
        self._running = False
        if self._task:
            await self._task
            self._task = None

    async def transcription_loop(self):
        print("âœ¨ Improved Whisperæ–‡å­—èµ·ã“ã—ãƒ«ãƒ¼ãƒ—èµ·å‹•")
        while self._running:
            audio_data = []
            silence_start = None
            image_at_trigger = None

            try:
                first_chunk = await asyncio.wait_for(
                    self.audio_queue.get(), timeout=0.5
                )
            except asyncio.TimeoutError:
                continue

            # åˆå›ãƒãƒ£ãƒ³ã‚¯ãŒç™ºè©±ã‹å¦ã‹ã§éŒ²éŸ³é–‹å§‹ã®åˆ¤æ–­
            if self.use_vad:
                is_speech = self.is_speech_vad(first_chunk[: self.frame_size])
            else:
                is_speech = (
                    self.compute_rms(first_chunk) > self.silence_rms_threshold
                )

            if not is_speech:
                continue

            print("ğŸ¤ éŸ³å£°æ¤œå‡ºã€éŒ²éŸ³é–‹å§‹")
            audio_data.append(first_chunk)
            image_at_trigger = self.latest_image_base64

            while self._running:
                try:
                    chunk = await asyncio.wait_for(
                        self.audio_queue.get(), timeout=0.1
                    )
                except asyncio.TimeoutError:
                    if (
                        silence_start
                        and time.time() - silence_start
                        > self.silence_duration_threshold
                    ):
                        print("ğŸ“´ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§éŒ²éŸ³çµ‚äº†")
                        break
                    continue

                if self.use_vad:
                    is_speech = self.is_speech_vad(chunk[: self.frame_size])
                else:
                    is_speech = (
                        self.compute_rms(chunk) > self.silence_rms_threshold
                    )

                if is_speech:
                    audio_data.append(chunk)
                    silence_start = None
                else:
                    if silence_start is None:
                        silence_start = time.time()
                    elif (
                        time.time() - silence_start
                        > self.silence_duration_threshold
                    ):
                        print("ğŸ“´ ç„¡éŸ³æ¤œå‡ºã€éŒ²éŸ³çµ‚äº†")
                        break

            if not audio_data:
                continue

            total_duration = (
                sum(len(chunk) for chunk in audio_data) / self.sample_rate
            )
            if total_duration < 0.5:
                print("âš ï¸ éŒ²éŸ³ãŒçŸ­ã™ãã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                continue

            audio_segment = np.concatenate(audio_data)
            print("ğŸ‹ï¸ Whisperã§æ–‡å­—èµ·ã“ã—é–‹å§‹")
            image_snapshot = image_at_trigger
            with NamedTemporaryFile(suffix=".wav", delete=True) as tmp_file:
                wav.write(
                    tmp_file.name,
                    self.sample_rate,
                    (audio_segment * 32767).astype(np.int16),
                )
                tmp_file.flush()
                result = self.model.transcribe(
                    tmp_file.name,
                    language="ja",
                    fp16=torch.cuda.is_available()
                )
                transcription_text = result["text"]
                print("ğŸ”¢ æ–‡å­—èµ·ã“ã—çµæœ:", transcription_text)
                # æ–‡å­—èµ·ã“ã—çµæœã‚’å‡ºåŠ›ã‚­ãƒ¥ãƒ¼ã«æŠ•å…¥
                await self.result_queue.put(transcription_text)  # audio-onlyç”¨
                await self.result_bundle_queue.put(
                    {  # imageå¯¾å¿œç”¨
                        "text": transcription_text,
                        "image": image_snapshot,
                    }
                )


def is_invalid_transcription(text: str, repeat_threshold: int = 5) -> bool:
    if not text.strip():
        return True  # ç©ºæ–‡å­—

    # å˜èªãƒ™ãƒ¼ã‚¹ã§ã®ç¹°ã‚Šè¿”ã—ã‚‚è©¦ã¿ã‚‹ï¼ˆæ—¥æœ¬èªå½¢æ…‹ç´ è§£æãªã—ã§æ–‡å­—nã‚°ãƒ©ãƒ ã§è¿‘ä¼¼ï¼‰
    ngram_lengths = [4, 6, 8]  # çŸ­ã€œä¸­ç¨‹åº¦ã®èªã®ç¹°ã‚Šè¿”ã—ã‚’ã‚«ãƒãƒ¼
    for n in ngram_lengths:
        chunks = [text[i : i + n] for i in range(0, len(text) - n + 1)]
        counts = Counter(chunks)
        for token, count in counts.items():
            if count >= repeat_threshold:
                print(f"ğŸš« ç¹°ã‚Šè¿”ã—æ¤œå‡º: '{token}' ãŒ {count} å›")
                return True

    # åŒä¸€æ–‡å­—ã®é€£ç¶šï¼ˆä¾‹: ã‚ã‚ã‚ã‚ã‚ã‚ï¼‰
    if re.search(r"(.)\1{6,}", text):
        return True

    return False
