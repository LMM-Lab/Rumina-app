import asyncio
import time
from tempfile import NamedTemporaryFile

import numpy as np
import scipy.io.wavfile as wav
import webrtcvad
import whisper
from numpy.typing import NDArray


class WhisperAudioTranscriber:
    def __init__(self, sample_rate: int = 16000, use_vad: bool = True):
        self.sample_rate = sample_rate
        self.model = whisper.load_model("base")
        # éåŒæœŸå‡¦ç†ç”¨ã® asyncio.Queue ã‚’ä½¿ç”¨
        self.audio_queue = asyncio.Queue()
        # æ–‡å­—èµ·ã“ã—çµæœã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é€ä¿¡ã™ã‚‹ãŸã‚ã®ã‚­ãƒ¥ãƒ¼
        self.result_queue = asyncio.Queue()
        self.use_vad = use_vad

        # RMS åˆ¤å®šç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.silence_rms_threshold = 0.01
        self.silence_duration_threshold = 0.3

        # VAD åˆ¤å®šç”¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆãƒ¢ãƒ¼ãƒ‰ 2: ä¸­ç¨‹åº¦ã®å³ã—ã•ï¼‰
        self.vad = webrtcvad.Vad(2)
        self.frame_duration_ms = 30  # 30ms
        self.frame_size = int(sample_rate * self.frame_duration_ms / 1000)

        self._running = False
        self._task: asyncio.Task[None] | None = None

    def compute_rms(self, data: NDArray[np.float32]) -> float:
        return np.sqrt(np.mean(np.square(data)))

    def is_speech_vad(self, pcm_chunk: NDArray[np.int16]) -> bool:
        if pcm_chunk.dtype != np.int16:
            pcm_chunk = (pcm_chunk * 32767).astype(np.int16)
        pcm_bytes = pcm_chunk.tobytes()
        try:
            return self.vad.is_speech(pcm_bytes, sample_rate=self.sample_rate)
        except Exception as e:
            print(f"VADã‚¨ãƒ©ãƒ¼: {e}")
            return False

    async def put_audio_chunk(self, chunk: bytes):
        np_chunk = (
            np.frombuffer(chunk, dtype=np.int16).astype(np.float32) / 32768.0
        )
        await self.audio_queue.put(np_chunk)

    async def start(self):
        if not self._running:
            print("â–¶ï¸ Improved transcription loop ã‚’é–‹å§‹")
            self._running = True
            self._task = asyncio.create_task(self.transcription_loop())

    async def stop(self):
        print("â›” Improved transcription loop ã‚’åœæ­¢")
        self._running = False
        if self._task:
            await self._task
            self._task = None

    async def transcription_loop(self):
        print("âœ¨ Improved Whisperæ–‡å­—èµ·ã“ã—ãƒ«ãƒ¼ãƒ—èµ·å‹•")
        while self._running:
            audio_data = []
            silence_start = None

            try:
                first_chunk = await asyncio.wait_for(
                    self.audio_queue.get(), timeout=0.5
                )
            except asyncio.TimeoutError:
                continue

            # åˆå›ãƒãƒ£ãƒ³ã‚¯ãŒç™ºè©±ã‹å¦ã‹ã§éŒ²éŸ³é–‹å§‹ã®åˆ¤æ–­
            if self.use_vad:
                is_speech = self.is_speech_vad(first_chunk[: self.frame_size])
            else:
                is_speech = (
                    self.compute_rms(first_chunk) > self.silence_rms_threshold
                )

            if not is_speech:
                continue

            print("ğŸ¤ éŸ³å£°æ¤œå‡ºã€éŒ²éŸ³é–‹å§‹")
            audio_data.append(first_chunk)

            while self._running:
                try:
                    chunk = await asyncio.wait_for(
                        self.audio_queue.get(), timeout=0.1
                    )
                except asyncio.TimeoutError:
                    if (
                        silence_start
                        and time.time() - silence_start
                        > self.silence_duration_threshold
                    ):
                        print("ğŸ“´ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§éŒ²éŸ³çµ‚äº†")
                        break
                    continue

                if self.use_vad:
                    is_speech = self.is_speech_vad(chunk[: self.frame_size])
                else:
                    is_speech = (
                        self.compute_rms(chunk) > self.silence_rms_threshold
                    )

                if is_speech:
                    audio_data.append(chunk)
                    silence_start = None
                else:
                    if silence_start is None:
                        silence_start = time.time()
                    elif (
                        time.time() - silence_start
                        > self.silence_duration_threshold
                    ):
                        print("ğŸ“´ ç„¡éŸ³æ¤œå‡ºã€éŒ²éŸ³çµ‚äº†")
                        break

            if not audio_data:
                continue

            total_duration = (
                sum(len(chunk) for chunk in audio_data) / self.sample_rate
            )
            if total_duration < 0.5:
                print("âš ï¸ éŒ²éŸ³ãŒçŸ­ã™ãã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                continue

            audio_segment = np.concatenate(audio_data)
            print("ğŸ‹ï¸ Whisperã§æ–‡å­—èµ·ã“ã—é–‹å§‹")
            with NamedTemporaryFile(suffix=".wav", delete=True) as tmp_file:
                wav.write(
                    tmp_file.name,
                    self.sample_rate,
                    (audio_segment * 32767).astype(np.int16),
                )
                tmp_file.flush()
                result = self.model.transcribe(tmp_file.name, language="ja")
                transcription_text = result["text"]
                print("ğŸ”¢ æ–‡å­—èµ·ã“ã—çµæœ:", transcription_text)
                # æ–‡å­—èµ·ã“ã—çµæœã‚’å‡ºåŠ›ã‚­ãƒ¥ãƒ¼ã«æŠ•å…¥
                await self.result_queue.put(transcription_text)
