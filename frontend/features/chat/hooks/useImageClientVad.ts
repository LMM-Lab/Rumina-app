import { useEffect, useRef, useState } from "react";

export type ChatMessage = {
    text: string;
    isUser: boolean;
};

export const useImageClientVad = (onStartSpeaking?: () => void, onStopSpeaking?: () => void) => {
    const instanceId = useRef(Math.random());
    const [isRecording, setIsRecording] = useState(false);
    const [transcriptions, setTranscriptions] = useState<ChatMessage[]>([]);
    const [isSpeaking, setIsSpeaking] = useState(false);

    const audioContextRef = useRef<AudioContext | null>(null);
    const mediaStreamRef = useRef<MediaStream | null>(null);
    const analyserRef = useRef<AnalyserNode | null>(null);
    const dataArrayRef = useRef<Uint8Array | null>(null);
    const vadIntervalRef = useRef<NodeJS.Timeout | null>(null);
    const workletNodeRef = useRef<AudioWorkletNode | null>(null);

    const speakingStartTimeRef = useRef<number | null>(null);
    const silenceStartTimeRef = useRef<number | null>(null);
    const isSpeakingRef = useRef(false);
    const speechFrameCountRef = useRef(0);

    // üéÅ ÊîπÂñÑÁâà„Éó„É™„É≠„Éº„É´
    type TimestampedFrame = {
        frame: Float32Array;
        timestamp: number;
    };
    const preRollBufferRef = useRef<TimestampedFrame[]>([]);

    const frameDurationMs = 32;
    const desiredPreRollMs = 3000;
    const maxPreRollFrames = Math.ceil(desiredPreRollMs / frameDurationMs);

    const socketRef = useRef<WebSocket | null>(null);

    const setSpeakingState = (val: boolean) => {
        isSpeakingRef.current = val;
        setIsSpeaking(val);
    };

    const sendAudioFrameToServer = (pcmFrame: Float32Array, type: string = "active_audio_chunk") => {
        if (!socketRef.current || socketRef.current.readyState !== WebSocket.OPEN) return;

        socketRef.current.send(JSON.stringify({ type }));

        const int16Frame = new Int16Array(pcmFrame.length);
        for (let i = 0; i < pcmFrame.length; i++) {
            int16Frame[i] = Math.max(-32768, Math.min(32767, pcmFrame[i] * 32767));
        }

        socketRef.current.send(int16Frame.buffer);
    };

    const stopRecording = () => {
        if (audioContextRef.current) audioContextRef.current.close();
        if (mediaStreamRef.current) {
            mediaStreamRef.current.getTracks().forEach((track) => track.stop());
        }
        if (vadIntervalRef.current) {
            clearInterval(vadIntervalRef.current);
            vadIntervalRef.current = null;
        }
        if (workletNodeRef.current) {
            workletNodeRef.current.disconnect();
            workletNodeRef.current = null;
        }
        if (socketRef.current) {
            socketRef.current.close();
            socketRef.current = null;
        }
        preRollBufferRef.current = [];

        setIsRecording(false);
        setIsSpeaking(false);
    };

    const captureCurrentFrame = async (): Promise<string> => {
        const video = document.querySelector("video") as HTMLVideoElement;
        if (!video || video.videoWidth === 0 || video.videoHeight === 0) {
            throw new Error("„Ç´„É°„É©Êò†ÂÉè„Åå‰ΩøÁî®„Åß„Åç„Åæ„Åõ„Çì");
        }
        const canvas = document.createElement("canvas");
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        const ctx = canvas.getContext("2d");
        if (!ctx) throw new Error("Canvas„ÅåÂàùÊúüÂåñ„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü");
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        return canvas.toDataURL("image/png");
    };

    const startRecording = async () => {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            mediaStreamRef.current = stream;

            const audioContext = new AudioContext({ sampleRate: 16000 });
            audioContextRef.current = audioContext;

            const source = audioContext.createMediaStreamSource(stream);
            const analyser = audioContext.createAnalyser();
            analyser.fftSize = 512;
            analyserRef.current = analyser;
            const dataArray = new Uint8Array(analyser.fftSize);
            dataArrayRef.current = dataArray;

            source.connect(analyser);

            const socket = new WebSocket("ws://localhost:8000/ws/m/image");
            socket.binaryType = "arraybuffer";
            socketRef.current = socket;

            socket.onopen = () => {
                socket.send(JSON.stringify({
                    type: "init",
                    model: "rumina-m1",
                    vad_silence_threshold: 1000,
                }));
                console.log("‚úÖ WebSocket Êé•Á∂ö & init „É°„ÉÉ„Çª„Éº„Ç∏ÈÄÅ‰ø°");
            };

            socket.onmessage = async (event) => {
                try {
                    const data = JSON.parse(event.data);
                    const { type, message, audio_base64 } = data;

                    console.log(`[Âèó‰ø°] type: ${type}, message: ${message}`);

                    if (message) {
                        setTranscriptions((prev) => {
                            const newMessage: ChatMessage = {
                                text: message,
                                isUser: type === "transcription",
                            };
                            return [...prev, newMessage];
                        });
                    }

                    if (audio_base64) {
                        const audioBlob = new Blob(
                            [Uint8Array.from(atob(audio_base64), (c) => c.charCodeAt(0))],
                            { type: "audio/wav" }
                        );
                        const audioUrl = URL.createObjectURL(audioBlob);
                        const audio = new Audio(audioUrl);
                        await audio.play().catch((err) => {
                            console.error("Èü≥Â£∞„ÅÆÂÜçÁîü„Å´Â§±Êïó„Åó„Åæ„Åó„Åü:", err);
                        });
                    }
                } catch (e) {
                    console.error("WebSocket„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆËß£Êûê„Å´Â§±Êïó:", e);
                }
            };

            await audioContext.audioWorklet.addModule('/worklet/pcm-processor.js');

            const workletNode = new AudioWorkletNode(audioContext, 'pcm-processor');
            workletNodeRef.current = workletNode;

            workletNode.port.onmessage = (event) => {
                if (event.data?.type === 'audio') {
                    const pcmFrame = event.data.pcm as Float32Array;
                    const now = Date.now();

                    // üéÅ timestamp‰ªò„Åç„Åß‰øùÂ≠ò
                    preRollBufferRef.current.push({ frame: pcmFrame, timestamp: now });
                    while (preRollBufferRef.current.length > maxPreRollFrames) {
                        preRollBufferRef.current.shift();
                    }

                    if (isSpeakingRef.current) {
                        sendAudioFrameToServer(pcmFrame, "active_audio_chunk");
                    }
                }
            };

            source.connect(workletNode);
            workletNode.connect(audioContext.destination);

            vadIntervalRef.current = setInterval(() => {
                if (!analyserRef.current || !dataArrayRef.current) return;
                analyserRef.current.getByteTimeDomainData(dataArrayRef.current);
                const rms = Math.sqrt(
                    dataArrayRef.current.reduce((sum, val) => sum + (val - 128) ** 2, 0) / dataArrayRef.current.length
                );
                const threshold = 5;
                const now = Date.now();

                const speechFrameThreshold = 3;

                if (rms > threshold) {
                    speechFrameCountRef.current++;
                    if (!isSpeakingRef.current && speechFrameCountRef.current >= speechFrameThreshold) {
                        console.log("[VAD] Ë©±„ÅóÂßã„ÇÅ„ÇíÁ¢∫ÂÆö");
                        setSpeakingState(true);
                        onStartSpeaking?.();
                        silenceStartTimeRef.current = null;

                        (async () => {
                            if (socketRef.current && socketRef.current.readyState === WebSocket.OPEN) {
                                try {
                                    const imageBase64 = await captureCurrentFrame();
                                    socketRef.current.send(JSON.stringify({
                                        type: "active_audio_start",
                                        image_base64: imageBase64,
                                    }));
                                    console.log("üì∑ ÂàùÂõû„Éï„É¨„Éº„É†ÈÄÅ‰ø°ÂÆå‰∫Ü");

                                    // üéÅ „Éó„É™„É≠„Éº„É´ÈÄÅ‰ø°
                                    const cutoffTime = Date.now() - desiredPreRollMs;
                                    const framesToSend = preRollBufferRef.current.filter(f => f.timestamp >= cutoffTime);
                                    console.log(`üì§ „Éó„É™„É≠„Éº„É´ÈÄÅ‰ø°: ${framesToSend.length} frames`);

                                    for (const f of framesToSend) {
                                        sendAudioFrameToServer(f.frame, "active_audio_chunk");
                                    }
                                } catch (err) {
                                    console.error("ÂàùÂõû„Éï„É¨„Éº„É†„ÅÆ„Ç≠„É£„Éó„ÉÅ„É£ÈÄÅ‰ø°Â§±Êïó:", err);
                                }
                            }
                        })();
                    }
                    if (isSpeakingRef.current) silenceStartTimeRef.current = null;
                } else {
                    if (!isSpeakingRef.current) {
                        speechFrameCountRef.current = 0;
                    } else {
                        if (!silenceStartTimeRef.current) silenceStartTimeRef.current = now;
                        if (now - silenceStartTimeRef.current > 1000) {
                            console.log("[VAD] Ë©±„ÅóÁµÇ„Çè„Çä„ÇíÁ¢∫ÂÆö");
                            setSpeakingState(false);
                            onStopSpeaking?.();

                            if (socketRef.current && socketRef.current.readyState === WebSocket.OPEN) {
                                socketRef.current.send(JSON.stringify({ type: "active_audio_end" }));
                            }

                            silenceStartTimeRef.current = null;
                        }
                    }
                }

            }, 100);

            setIsRecording(true);
        } catch (error) {
            console.error("Èå≤Èü≥„ÅÆÈñãÂßã„Å´Â§±Êïó„Åó„Åæ„Åó„Åü:", error);
        }
    };

    useEffect(() => {
        return () => {
            stopRecording();
        };
    }, []);

    const toggleRecording = () => {
        if (isRecording) {
            stopRecording();
        } else {
            startRecording();
        }
    };

    return {
        instanceId: instanceId.current,
        isRecording,
        isSpeaking,
        toggleRecording,
        transcriptions,
    };
};
